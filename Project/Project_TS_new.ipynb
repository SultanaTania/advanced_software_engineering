{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read text file\n",
    "def read_text_file(path):\n",
    "    book = \"\"\n",
    "    with open(path,  encoding=\"utf8\") as f:\n",
    "        #read and remove empty space from the last of the text\n",
    "        book = f.read().strip()\n",
    "    return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(word):\n",
    "    return [char for char in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the special character from the file\n",
    "def remove_special_characters(file):\n",
    "    \n",
    "    modified_file = file\n",
    "   \n",
    "    # special characters\n",
    "    special_characters = \"\\\"\\\"\\'\\'\\“\\”#$%&()*+,-./:;!<=>?@[\\]^_`{|}~\"\n",
    "    special_char = splitter(special_characters)\n",
    " \n",
    "    # remove each special characters from the input file\n",
    "    for sp_char in special_char:\n",
    "        modified_file = modified_file.replace(sp_char, \" \")\n",
    "    \n",
    "    return modified_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the new line from the file\n",
    "def remove_new_lines(file):\n",
    "    return file.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all the characters to lower case\n",
    "def convert_to_lowercase(file):\n",
    "    return file.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the text into list of words\n",
    "def text_to_word(text):\n",
    "    words = text.split()\n",
    "    \n",
    "    words = [word for word in words if len(word.strip()) > 0]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert the list of word to a dictionary with unique words and frequency\n",
    "def uniq_word_with_freq(words):\n",
    "    dict = {}\n",
    "    \n",
    "    for word in words:\n",
    "        if word in dict:\n",
    "            dict[word] += 1\n",
    "        else:\n",
    "            dict[word] = 1\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop word are the words which is used most commonly in a language\n",
    "def stop_word_percentage(word_dict):\n",
    "    \n",
    "    # get english stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # get all words in the document\n",
    "    total_word = sum(word_dict.values())\n",
    "    \n",
    "    # stop word occurence\n",
    "    sw_counts = sum([v for (k,v) in word_dict.items() if k in stop_words])\n",
    "     \n",
    "    occurance = str(round((sw_counts / total_word) * 100, 2)) + \"%\"\n",
    "    \n",
    "    return occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_square(data):\n",
    "    return sum([d*d for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_calculate(df1, df2):\n",
    "    \n",
    "    df1.rename(columns={'frequency': 'tf'}, inplace=True)\n",
    "    df2.rename(columns={'frequency': 'tf'}, inplace=True)\n",
    "\n",
    "    # tf and df of doc-1\n",
    "    tf_df_1 =  df1['tf'].tolist()\n",
    "\n",
    "    tf_idf1 = []\n",
    "    for d in tf_df_1:\n",
    "        # idf = log2(n/df)\n",
    "        tf_idf1.append(math.log2(1/d) * d)\n",
    "\n",
    "    df1[\"tf_idf\"] = tf_idf1\n",
    "    \n",
    "    tf_2 =  df2['tf'].tolist()\n",
    "    tf_idf2 = []\n",
    "\n",
    "    for d in range(len(df2)):\n",
    "        tf = df2['tf'].iloc[d]\n",
    "\n",
    "        df_l = df1.tf[df1.unique_words == df2['unique_words'].iloc[d]]\n",
    "\n",
    "        df = df_l.values[0] if (len(df_l) > 0) else 0\n",
    "\n",
    "        tf_idf = (tf * math.log2(1/df)) if (df!=0) else 0\n",
    "\n",
    "        tf_idf2.append(tf_idf)\n",
    "\n",
    "    df2[\"tf_idf\"] = tf_idf2\n",
    "\n",
    "    all_sum = 0\n",
    "    # get the scaler product of both tf_idf\n",
    "    for d in range(len(df1)):\n",
    "        tf_idf1 = df1['tf_idf'].iloc[d]\n",
    "\n",
    "        tf_idf2 = df2.tf_idf[df2.unique_words == df1['unique_words'].iloc[d]]\n",
    "\n",
    "        mul = tf_idf1 * tf_idf2.values[0] if (len(tf_idf2) > 0) else 0\n",
    "        all_sum += mul\n",
    "\n",
    "    # get sum square of doc-1\n",
    "    sm1 = math.sqrt(sum_of_square(df1.tf_idf.values))\n",
    "    sm2 = math.sqrt(sum_of_square(df2.tf_idf.values))\n",
    "\n",
    "    rad = all_sum / (sm1 * sm2)\n",
    "\n",
    "    theta = str(round( math.degrees(math.acos(rad)), 2)) + \"%\"\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word in document1: 1489\n",
      "Total uniqe word in document1: 632\n",
      "Total word in document2: 2531\n",
      "Total uniqe word in document2: 905\n",
      "\n",
      "Top ten word occured in document-1 are: \n",
      "unique_words  frequency\n",
      "         the         78\n",
      "          to         48\n",
      "           a         39\n",
      "          in         37\n",
      "         and         28\n",
      "          of         27\n",
      "          is         23\n",
      "          we         17\n",
      "         has         15\n",
      "        that         15\n",
      "\n",
      "Top ten word occured in document-2 are: \n",
      "unique_words  frequency\n",
      "         the        137\n",
      "          of         84\n",
      "           a         71\n",
      "          in         65\n",
      "          to         59\n",
      "         and         54\n",
      "          he         33\n",
      "          an         29\n",
      "        that         28\n",
      "          is         28\n",
      "\n",
      "\n",
      "Stop word occurance in document-1: 40.03%\n",
      "Stop word occurance in document-2: 43.82%\n",
      "\n",
      "\n",
      "Both document has a similarity of 16.64%\n"
     ]
    }
   ],
   "source": [
    "def main_process():\n",
    "    \n",
    "    # read and clean doc 1\n",
    "    file1 = convert_to_lowercase(remove_new_lines(remove_special_characters\n",
    "                                                  (read_text_file('data/doc1.txt'))))\n",
    "    \n",
    "    # all words of doc 1\n",
    "    words1 = text_to_word(file1)\n",
    "    print(f'Total word in document1: {len(words1)}')\n",
    "    \n",
    "    # get freq of doc 1\n",
    "    dict1 = uniq_word_with_freq(words1)\n",
    "    print(f'Total uniqe word in document1: {len(dict1)}')\n",
    "    \n",
    "    \n",
    "    # second file read\n",
    "    file2 = convert_to_lowercase(remove_new_lines(remove_special_characters\n",
    "                                                  (read_text_file('data/doc2.txt'))))\n",
    "    \n",
    "    # all words of doc 2\n",
    "    words2 = text_to_word(file2)\n",
    "    print(f'Total word in document2: {len(words2)}')\n",
    "    \n",
    "    # get freq of doc 2\n",
    "    dict2 = uniq_word_with_freq(words2)\n",
    "    print(f'Total uniqe word in document2: {len(dict2)}')\n",
    "\n",
    "    \n",
    "    # first document dict to data frame\n",
    "    df1 = pd.DataFrame({\n",
    "        \"unique_words\" : dict1.keys(),\n",
    "        \"frequency\" : dict1.values()\n",
    "    })\n",
    "    \n",
    "    # sort them using their frequency \n",
    "    df1 = df1.sort_values(by=['frequency'], ascending=False)\n",
    "\n",
    "    # top 10 data of document 1\n",
    "    df1_header = df1.head(10).to_string(index=False)\n",
    "    print(f'\\nTop ten word occured in document-1 are: \\n{df1_header}')\n",
    "    \n",
    "    \n",
    "    # second document dict to data frame\n",
    "    df2 = pd.DataFrame({\n",
    "        \"unique_words\" : dict2.keys(),\n",
    "        \"frequency\" : dict2.values()\n",
    "    })\n",
    "    \n",
    "    # sort them using their frequency \n",
    "    df2 = df2.sort_values(by=['frequency'], ascending=False)\n",
    "\n",
    "    # top 10 data of document 1\n",
    "    df2_header = df2.head(10).to_string(index=False)\n",
    "    print(f'\\nTop ten word occured in document-2 are: \\n{df2_header}')\n",
    "    \n",
    "    # stop word occurance\n",
    "    swp1 = stop_word_percentage(dict1)\n",
    "    swp2 = stop_word_percentage(dict2)\n",
    "    \n",
    "    print(f'\\n\\nStop word occurance in document-1: {swp1}')\n",
    "    print(f'Stop word occurance in document-2: {swp2}')\n",
    "    \n",
    "    similarity = tf_idf_calculate(df1, df2)\n",
    "    print(f'\\n\\nBoth document has a similarity of {similarity}')\n",
    "    \n",
    "    return\n",
    "\n",
    "main_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
